# spark_apps/spark_scd2_processor.py

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import from_json, col, lit, current_timestamp, row_number
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType, DecimalType
from pyspark.sql.window import Window
from delta import DeltaTable

# --- C·∫•u h√¨nh ---
MINIO_ENDPOINT = "http://minio:9000"
MINIO_ACCESS_KEY = "minioadmin"
MINIO_SECRET_KEY = "minioadmin"

# --- Schemas cho c√°c b·∫£ng ---
customers_schema = StructType([
    StructField("CustomerID", IntegerType()), StructField("Name", StringType()), StructField("Email", StringType()),
    StructField("PhoneNumber", StringType()), StructField("CreatedAt", LongType()), StructField("UpdatedAt", LongType())
])

orders_schema = StructType([
    StructField("OrderID", IntegerType()), StructField("OrderNumber", StringType()), StructField("TotalAmount", DecimalType(18, 2)),
    StructField("StatusID", IntegerType()), StructField("CustomerID", IntegerType()), StructField("CreatedAt", LongType()),
    StructField("UpdatedAt", LongType())
])

order_items_schema = StructType([
    StructField("OrderItemID", IntegerType()), StructField("OrderID", IntegerType()), StructField("ProductID", IntegerType()),
    StructField("Quantity", IntegerType()), StructField("CurrentPrice", DecimalType(18, 2)), StructField("CreatedAt", LongType()),
    StructField("UpdatedAt", LongType())
])

products_schema = StructType([
    StructField("ProductID", IntegerType()), StructField("Name", StringType()), StructField("Description", StringType()),
    StructField("Price", DecimalType(18, 2)), StructField("CategoryID", IntegerType()), StructField("SellerID", IntegerType()),
    StructField("CreatedAt", LongType()), StructField("UpdatedAt", LongType())
])

inventory_schema = StructType([
    StructField("InventoryID", IntegerType()), StructField("InventoryName", StringType()), StructField("ProductID", IntegerType()),
    StructField("QuantityInStock", IntegerType()), StructField("ReorderThreshold", IntegerType()), StructField("UnitCost", DecimalType(18, 2)),
    StructField("CreatedAt", LongType()), StructField("UpdatedAt", LongType())
])

# Schema ƒë·∫ßy ƒë·ªß c·ªßa Debezium
def get_debezium_schema(data_schema):
    return StructType([
        StructField("payload", StructType([
            StructField("before", data_schema),
            StructField("after", data_schema),
            StructField("op", StringType()),
            StructField("ts_ms", LongType())
        ]))
    ])

def configure_spark_session():
    return SparkSession.builder \
        .appName("RealtimePipelineProcessor") \
        .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,io.delta:delta-spark_2.12:3.1.0,org.apache.hadoop:hadoop-aws:3.3.4") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.hadoop.fs.s3a.endpoint", MINIO_ENDPOINT) \
        .config("spark.hadoop.fs.s3a.access.key", MINIO_ACCESS_KEY) \
        .config("spark.hadoop.fs.s3a.secret.key", MINIO_SECRET_KEY) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.driver.host", "spark-master") \
        .getOrCreate()

def upsert_scd2_customers(micro_batch_df: DataFrame, batch_id: int):
    """
    H√†m x·ª≠ l√Ω logic SCD2 cho b·∫£ng Customers, bao g·ªìm t·∫°o b·∫£ng, ƒë√≥ng b·∫£n ghi c≈©,
    v√† ch√®n b·∫£n ghi m·ªõi.
    """
    print(f"--- B·∫Øt ƒë·∫ßu x·ª≠ l√Ω SCD2 Customers Batch ID: {batch_id} ---")
    
    DELTA_TABLE_PATH = "s3a://datalake/customers_scd2"
    spark = micro_batch_df.sparkSession

    if micro_batch_df.rdd.isEmpty():
        print("Batch Customers r·ªóng, b·ªè qua.")
        return

    # 1. Ph√¢n t√≠ch payload c·ªßa Debezium
    debezium_payload_df = micro_batch_df.select(
        from_json(col("value").cast("string"), get_debezium_schema(customers_schema)).alias("message")
    ).select("message.payload.*")

    source_changes_raw = debezium_payload_df.filter(col("op").isNotNull())
    if source_changes_raw.rdd.isEmpty():
        print("Batch Customers kh√¥ng ch·ª©a thay ƒë·ªïi h·ª£p l·ªá, b·ªè qua.")
        return

    # 2. Lo·∫°i b·ªè c√°c s·ª± ki·ªán tr√πng l·∫∑p trong c√πng m·ªôt batch
    window_spec = Window.partitionBy(col("after.CustomerID")).orderBy(col("ts_ms").desc())
    source_changes = source_changes_raw.withColumn("rank", row_number().over(window_spec)) \
                                       .filter(col("rank") == 1) \
                                       .drop("rank")
    source_changes.cache()

    # 3. X·ª≠ l√Ω logic ƒë√≥ng b·∫£n ghi c≈© n·∫øu b·∫£ng ƒë√£ t·ªìn t·∫°i
    if DeltaTable.isDeltaTable(spark, DELTA_TABLE_PATH):
        updates_and_deletes = source_changes.filter(col("op").isin(['u', 'd']))
        if not updates_and_deletes.rdd.isEmpty():
            keys_to_expire = updates_and_deletes.selectExpr(
                "nvl(after.CustomerID, before.CustomerID) as CustomerID"
            ).distinct()
            print(f"S·∫Ω ƒë√≥ng {keys_to_expire.count()} b·∫£n ghi Customers c≈©...")
            
            delta_table = DeltaTable.forPath(spark, DELTA_TABLE_PATH)
            delta_table.alias("target").merge(
                source=keys_to_expire.alias("source"),
                condition="target.CustomerID = source.CustomerID AND target.is_current = true"
            ).whenMatchedUpdate(
                set={
                    "is_current": lit(False),
                    "end_date": current_timestamp()
                }
            ).execute()
    else:
        print("B·∫£ng Customers Delta ch∆∞a t·ªìn t·∫°i. S·∫Ω t·∫°o b·∫£ng m·ªõi.")

    # 4. Chu·∫©n b·ªã v√† ch√®n c√°c b·∫£n ghi m·ªõi (t·ª´ s·ª± ki·ªán CREATE v√† UPDATE)
    new_records_df = source_changes.filter(col("op").isin(['c', 'u'])).select("after.*")
    if not new_records_df.rdd.isEmpty():
        new_records_to_insert = new_records_df.withColumn(
            "source_created_at", (col("CreatedAt") / 1000000000).cast(TimestampType())
        ).withColumn(
            "source_updated_at", (col("UpdatedAt") / 1000000000).cast(TimestampType())
        ).drop("CreatedAt", "UpdatedAt") \
        .withColumn("is_current", lit(True)) \
        .withColumn("effective_date", current_timestamp()) \
        .withColumn("end_date", lit(None).cast(TimestampType()))
        print(f"S·∫Ω ch√®n {new_records_to_insert.count()} b·∫£n ghi Customers m·ªõi...")
        new_records_to_insert.write.format("delta").mode("append").save(DELTA_TABLE_PATH)

    source_changes.unpersist()
    print(f"--- Ho√†n th√†nh x·ª≠ l√Ω Customers Batch ID: {batch_id} ---")

# REPLACE THE OLD FUNCTION WITH THIS NEW ONE

def upsert_simple_table(micro_batch_df: DataFrame, batch_id: int, table_name: str, schema: StructType, p_key: str):
    """H√†m generic ƒë·ªÉ upsert cho c√°c b·∫£ng ƒë∆°n gi·∫£n, ƒë√£ ƒë∆∞·ª£c th√™m logic lo·∫°i b·ªè tr√πng l·∫∑p."""
    print(f"--- B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {table_name} Batch ID: {batch_id} ---")
    
    path = f"s3a://datalake/{table_name.lower()}"
    spark = micro_batch_df.sparkSession
    
    debezium_payload_df = micro_batch_df.select(
        from_json(col("value").cast("string"), get_debezium_schema(schema)).alias("message")
    ).select("message.payload.*")
    
    source_changes_raw = debezium_payload_df.filter(col("op").isNotNull()) \
                                            .filter(col(f"after.{p_key}").isNotNull())

    if source_changes_raw.rdd.isEmpty():
        print(f"Batch {table_name} kh√¥ng c√≥ thay ƒë·ªïi h·ª£p l·ªá.")
        return

    # --- B∆Ø·ªöC QUAN TR·ªåNG: LO·∫†I B·ªé TR√ôNG L·∫∂P ---
    # Ch·ªâ gi·ªØ l·∫°i s·ª± ki·ªán cu·ªëi c√πng cho m·ªói Primary Key trong batch n√†y
    window_spec = Window.partitionBy(col(f"after.{p_key}")).orderBy(col("ts_ms").desc())
    
    changes = source_changes_raw.withColumn("rank", row_number().over(window_spec)) \
                                .filter(col("rank") == 1) \
                                .drop("rank") \
                                .select("after.*")

    if changes.rdd.isEmpty():
        print(f"Batch {table_name} kh√¥ng c√≥ thay ƒë·ªïi sau khi lo·∫°i b·ªè tr√πng l·∫∑p.")
        return

    # T·ª± t·∫°o b·∫£ng n·∫øu ch∆∞a c√≥
    if not DeltaTable.isDeltaTable(spark, path):
        print(f"ƒê√£ t·∫°o b·∫£ng Delta m·ªõi cho {table_name}.")
        changes.write.format("delta").mode("overwrite").save(path)
        delta_table = DeltaTable.forPath(spark, path)
    else:
        delta_table = DeltaTable.forPath(spark, path)

    # Th·ª±c hi·ªán merge
    delta_table.alias("target").merge(
        source=changes.alias("source"),
        condition=f"target.{p_key} = source.{p_key}"
    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()
    print(f"--- Ho√†n th√†nh x·ª≠ l√Ω {table_name} Batch ID: {batch_id} ---")
if __name__ == '__main__':
    spark = configure_spark_session()
    spark.sparkContext.setLogLevel("ERROR")

    topics = [
        "server_docker.testdb.dbo.Customers",
        "server_docker.testdb.dbo.Orders",
        "server_docker.testdb.dbo.OrderItems",
        "server_docker.testdb.dbo.Products",
        "server_docker.testdb.dbo.Inventory"
    ]

    kafka_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "kafka:29092") \
        .option("subscribe", ",".join(topics)) \
        .option("startingOffsets", "earliest") \
        .load()

    # T·∫°o c√°c stream ri√™ng cho t·ª´ng b·∫£ng
    
    # 1. Stream cho Customers (SCD2)
    # customer_stream = kafka_df.filter(col("topic") == topics[0]).writeStream \
    #     .foreachBatch(upsert_scd2_customers) \
    #     .outputMode("update") \
    #     .option("checkpointLocation", "/tmp/delta/checkpoints/customers_scd2") \
    #     .start()

    # 2. Stream cho Orders
    orders_stream = kafka_df.filter(col("topic") == topics[1]).writeStream \
        .foreachBatch(lambda df, batch_id: upsert_simple_table(df, batch_id, "Orders", orders_schema, "OrderID")) \
        .outputMode("update") \
        .option("checkpointLocation", "/tmp/delta/checkpoints/orders") \
        .start()

    # 3. Stream cho OrderItems
    order_items_stream = kafka_df.filter(col("topic") == topics[2]).writeStream \
        .foreachBatch(lambda df, batch_id: upsert_simple_table(df, batch_id, "OrderItems", order_items_schema, "OrderItemID")) \
        .outputMode("update") \
        .option("checkpointLocation", "/tmp/delta/checkpoints/order_items") \
        .start()
        
    # 4. Stream cho Products
    products_stream = kafka_df.filter(col("topic") == topics[3]).writeStream \
        .foreachBatch(lambda df, batch_id: upsert_simple_table(df, batch_id, "Products", products_schema, "ProductID")) \
        .outputMode("update") \
        .option("checkpointLocation", "/tmp/delta/checkpoints/products") \
        .start()

    # 5. Stream cho Inventory
    inventory_stream = kafka_df.filter(col("topic") == topics[4]).writeStream \
        .foreachBatch(lambda df, batch_id: upsert_simple_table(df, batch_id, "Inventory", inventory_schema, "InventoryID")) \
        .outputMode("update") \
        .option("checkpointLocation", "/tmp/delta/checkpoints/inventory") \
        .start()

    print("üöÄ B·∫Øt ƒë·∫ßu ch·∫°y c√°c pipeline streaming...")
    spark.streams.awaitAnyTermination()