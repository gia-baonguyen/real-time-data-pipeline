# spark_apps/spark_reader.py

import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# --- C·∫•u h√¨nh (ph·∫£i kh·ªõp v·ªõi k·ªãch b·∫£n ghi) ---
DELTA_TABLE_PATH = "s3a://datalake/customers_scd2"
MINIO_ENDPOINT = "http://minio:9000"
MINIO_ACCESS_KEY = "minioadmin"
MINIO_SECRET_KEY = "minioadmin"

def configure_spark_session():
    """C·∫•u h√¨nh Spark Session ƒë·ªÉ ƒë·ªçc Delta Lake t·ª´ MinIO."""
    return SparkSession.builder \
        .appName("DeltaTableReader") \
        .config("spark.jars.packages", "io.delta:delta-spark_2.12:3.1.0,org.apache.hadoop:hadoop-aws:3.3.4") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.hadoop.fs.s3a.endpoint", MINIO_ENDPOINT) \
        .config("spark.hadoop.fs.s3a.access.key", MINIO_ACCESS_KEY) \
        .config("spark.hadoop.fs.s3a.secret.key", MINIO_SECRET_KEY) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .getOrCreate()

def main():
    """
    H√†m ch√≠nh ƒë·ªÉ ƒë·ªçc v√† hi·ªÉn th·ªã d·ªØ li·ªáu t·ª´ b·∫£ng Delta.
    """
    spark = configure_spark_session()
    spark.sparkContext.setLogLevel("ERROR") # Ch·ªâ hi·ªán l·ªói ƒë·ªÉ d·ªÖ nh√¨n
    
    print(f"üìñ ƒêang ƒë·ªçc t·ª´ b·∫£ng Delta t·∫°i: {DELTA_TABLE_PATH}")
    
    try:
        delta_df = spark.read.format("delta").load(DELTA_TABLE_PATH)
        print("‚úÖ ƒê·ªçc b·∫£ng th√†nh c√¥ng.")
    except Exception as e:
        if "Path does not exist" in str(e):
            print(f"‚ùå L·ªói: ƒê∆∞·ªùng d·∫´n '{DELTA_TABLE_PATH}' kh√¥ng t·ªìn t·∫°i.")
            print("H√£y ƒë·∫£m b·∫£o k·ªãch b·∫£n streaming (spark_scd2_processor.py) ƒë√£ ch·∫°y v√† t·∫°o ra d·ªØ li·ªáu.")
            spark.stop()
            return
        else:
            raise e

    # Ki·ªÉm tra xem ng∆∞·ªùi d√πng c√≥ cung c·∫•p CustomerID l√†m tham s·ªë kh√¥ng
    if len(sys.argv) > 1:
        try:
            customer_id_to_check = int(sys.argv[1])
            print(f"\nüîç L·ªçc l·ªãch s·ª≠ cho CustomerID: {customer_id_to_check}")
            
            result_df = delta_df.filter(col("CustomerID") == customer_id_to_check) \
                                .orderBy(col("effective_date"))
            
            if result_df.rdd.isEmpty():
                 print("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu cho CustomerID n√†y.")
            else:
                 result_df.show(truncate=False)

        except ValueError:
            print(f"L·ªói: '{sys.argv[1]}' kh√¥ng ph·∫£i l√† m·ªôt CustomerID h·ª£p l·ªá. Vui l√≤ng nh·∫≠p m·ªôt s·ªë.")
    else:
        print("\nüìö Hi·ªÉn th·ªã 10 d√≤ng ƒë·∫ßu ti√™n c·ªßa to√†n b·ªô b·∫£ng l·ªãch s·ª≠...")
        print("(ƒê·ªÉ xem l·ªãch s·ª≠ c·ªßa m·ªôt kh√°ch h√†ng c·ª• th·ªÉ, h√£y ch·∫°y: spark-submit <t√™n_file> <customer_id>)")
        delta_df.orderBy("CustomerID", "effective_date").show(10, truncate=False)

    spark.stop()

if __name__ == '__main__':
    main()